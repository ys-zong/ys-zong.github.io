---
---

@article{zong2024safety,
  title={Safety Fine-Tuning at (Almost) No Cost: A Baseline for Vision Large Language Models},
  author={Zong, Yongshuo and Bohdal, Ondrej and Yu, Tingyang and Yang, Yongxin and Hospedales, Timothy},
  journal={ICML},
  year={2024},
  selected={true},
  code={https://github.com/ys-zong/VLGuard},
  arxiv={2402.02207},
  website={https://ys-zong.github.io/VLGuard/},
  preview={zong2024safety.png},
  tldr={VLLM fine-tuning breaks LLM safety, but our VLGuard can fix this.}
}

@article{zong2024fool,
  title={Fool your (vision and) language model with embarrassingly simple permutations},
  author={Zong, Yongshuo and Yu, Tingyang and Zhao, Bingchen and Chavhan, Ruchika and Hospedales, Timothy},
  journal={ICML},
  year={2024},
  selected={true},
  code={https://github.com/ys-zong/FoolyourVLLMs},
  arxiv={2310.01651},
  preview={zong2024fool.png},
  tldr={(V)LLM-based MCQ is not permutation robust.}
}

@article{zhang2024if,
  title={What if the tv was off? examining counterfactual reasoning abilities of multi-modal language models},
  author={Zhang, Letian and Zhai, Xiaotong and Zhao, Zhongkai and Zong, Yongshuo and Wen, Xin and Zhao, Bingchen},
  journal={CVPR},
  year={2024},
  selected={true},
  code={https://github.com/letian2003/c-vqa},
  website={https://bzhao.me/C-VQA/},
  arxiv={2310.06627},
  preview={zhang2024if.png},
  tldr={Vision large language models do not understand counterfactual conditions well.}
}

@article{bohdal2023meta,
  title={Meta omnium: A benchmark for general-purpose learning-to-learn},
  author={Bohdal, Ondrej and Tian, Yinbing and Zong, Yongshuo and Chavhan, Ruchika and Li, Da and Gouk, Henry and Guo, Li and Hospedales, Timothy},
  journal={CVPR},
  year={2023},
  selected={true},
  code={https://github.com/edi-meta-learning/meta-omnium},
  arxiv={2305.07625v1},
  website={https://edi-meta-learning.github.io/meta-omnium/},
  preview={bohdal2023meta.png},
  tldr={A framework for evaluating meta-learners across various vision tasks consistently.}
}

@article{zong2023medfair,
  title={MEDFAIR: benchmarking fairness for medical imaging},
  author={Zong, Yongshuo and Yang, Yongxin and Hospedales, Timothy},
  journal={ICLR},
  year={2023},
  selected={true},
  code={https://github.com/ys-zong/MEDFAIR},
  arxiv={2210.01725},
  website={https://ys-zong.github.io/MEDFAIR/},
  preview={zong2023medfair.png},
  tldr={We develop a fairness benchmark for medical imaging and find that the state-of-the-art bias mitigation algorithm does not significantly outperform ERM.}
}

@article{zong2024vl,
  title={VL-ICL Bench: The Devil in the Details of Benchmarking Multimodal In-Context Learning},
  author={Zong, Yongshuo and Bohdal, Ondrej and Hospedales, Timothy},
  journal={arXiv preprint},
  year={2024},
  selected={true},
  code={https://github.com/ys-zong/VL-ICL},
  arxiv={2403.13164},
  website={https://ys-zong.github.io/VL-ICL/},
  preview={zong2024vl.png},
  tldr={VL-ICL Bench is a better multimodal ICL benchmark than VQA and captioning.}
}

@article{zong2023self,
  title={Self-supervised multimodal learning: A survey},
  author={Zong, Yongshuo and Mac Aodha, Oisin and Hospedales, Timothy},
  journal={arXiv preprint},
  year={2023},
  selected={true},
  code={https://github.com/ys-zong/awesome-self-supervised-multimodal-learning},
  arxiv={2304.01008},
  preview={zong2023self.png},
  tldr={Systematic review of self-supervised multimodal learning methods.}
}

@article{zong2022const,
  title={conST: an interpretable multi-modal contrastive learning framework for spatial transcriptomics},
  author={Zong, Yongshuo and Yu, Tingyang and Wang, Xuesong and Wang, Yixuan and Hu, Zhihang and Li, Yu},
  journal={BioRxiv preprint},
  year={2022},
  selected={true},
  code={https://github.com/ys-zong/conST},
  html={https://www.biorxiv.org/content/10.1101/2022.01.14.476408v1},
  preview={zong2022const.png},
  tldr={A contrastive SSL method for spatial transcriptomics representation learning.}
}

