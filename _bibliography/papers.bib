---
---

@article{zong2024safety,
  title={Safety Fine-Tuning at (Almost) No Cost: A Baseline for Vision Large Language Models},
  author={Zong, Yongshuo and Bohdal, Ondrej and Yu, Tingyang and Yang, Yongxin and Hospedales, Timothy},
  journal={ICML},
  year={2024}
}

@article{zong2024fool,
  title={Fool your (vision and) language model with embarrassingly simple permutations},
  author={Zong, Yongshuo and Yu, Tingyang and Zhao, Bingchen and Chavhan, Ruchika and Hospedales, Timothy},
  journal={ICML},
  year={2024}
}

@inproceedings{zhang2024if,
  title={What if the tv was off? examining counterfactual reasoning abilities of multi-modal language models},
  author={Zhang, Letian and Zhai, Xiaotong and Zhao, Zhongkai and Zong, Yongshuo and Wen, Xin and Zhao, Bingchen},
  booktitle={CVPR},
  year={2024}
}

@inproceedings{bohdal2023meta,
  title={Meta omnium: A benchmark for general-purpose learning-to-learn},
  author={Bohdal, Ondrej and Tian, Yinbing and Zong, Yongshuo and Chavhan, Ruchika and Li, Da and Gouk, Henry and Guo, Li and Hospedales, Timothy},
  booktitle={CVPR},
  year={2023}
}

@article{zong2023medfair,
  title={MEDFAIR: benchmarking fairness for medical imaging},
  author={Zong, Yongshuo and Yang, Yongxin and Hospedales, Timothy},
  journal={ICLR},
  year={2023}
}

@article{zong2024vl,
  title={VL-ICL Bench: The Devil in the Details of Benchmarking Multimodal In-Context Learning},
  author={Zong, Yongshuo and Bohdal, Ondrej and Hospedales, Timothy},
  journal={arXiv preprint arXiv:2403.13164},
  year={2024}
}

@article{zong2023self,
  title={Self-supervised multimodal learning: A survey},
  author={Zong, Yongshuo and Mac Aodha, Oisin and Hospedales, Timothy},
  journal={arXiv preprint arXiv:2304.01008},
  year={2023}
}

@article{zong2022const,
  title={conST: an interpretable multi-modal contrastive learning framework for spatial transcriptomics},
  author={Zong, Yongshuo and Yu, Tingyang and Wang, Xuesong and Wang, Yixuan and Hu, Zhihang and Li, Yu},
  journal={BioRxiv preprint},
  year={2022},
}

