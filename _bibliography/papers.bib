---
---

@article{zong2024safety,
  title={Safety Fine-Tuning at (Almost) No Cost: A Baseline for Vision Large Language Models},
  author={Zong, Yongshuo and Bohdal, Ondrej and Yu, Tingyang and Yang, Yongxin and Hospedales, Timothy},
  journal={ICML},
  year={2024},
  selected={true},
  code={https://github.com/ys-zong/VLGuard},
  arxiv={https://arxiv.org/abs/2402.02207},
  website={https://ys-zong.github.io/VLGuard/}
}

@article{zong2024fool,
  title={Fool your (vision and) language model with embarrassingly simple permutations},
  author={Zong, Yongshuo and Yu, Tingyang and Zhao, Bingchen and Chavhan, Ruchika and Hospedales, Timothy},
  journal={ICML},
  year={2024},
  selected={true},
  code={https://github.com/ys-zong/FoolyourVLLMs},
  arxiv={https://arxiv.org/abs/2310.01651}
}

@inproceedings{zhang2024if,
  title={What if the tv was off? examining counterfactual reasoning abilities of multi-modal language models},
  author={Zhang, Letian and Zhai, Xiaotong and Zhao, Zhongkai and Zong, Yongshuo and Wen, Xin and Zhao, Bingchen},
  booktitle={CVPR},
  year={2024},
  selected={true},
  code={https://github.com/letian2003/c-vqa},
  arxiv={https://arxiv.org/abs/2310.06627}
}

@inproceedings{bohdal2023meta,
  title={Meta omnium: A benchmark for general-purpose learning-to-learn},
  author={Bohdal, Ondrej and Tian, Yinbing and Zong, Yongshuo and Chavhan, Ruchika and Li, Da and Gouk, Henry and Guo, Li and Hospedales, Timothy},
  booktitle={CVPR},
  year={2023},
  selected={true},
  code={https://github.com/edi-meta-learning/meta-omnium},
  arxiv={https://arxiv.org/abs/2305.07625v1},
  website={https://edi-meta-learning.github.io/meta-omnium/}
}

@article{zong2023medfair,
  title={MEDFAIR: benchmarking fairness for medical imaging},
  author={Zong, Yongshuo and Yang, Yongxin and Hospedales, Timothy},
  journal={ICLR},
  year={2023},
  selected={true},
  code={https://github.com/ys-zong/MEDFAIR},
  arxiv={https://arxiv.org/abs/2210.01725},
  website={https://ys-zong.github.io/MEDFAIR/}
}

@article{zong2024vl,
  title={VL-ICL Bench: The Devil in the Details of Benchmarking Multimodal In-Context Learning},
  author={Zong, Yongshuo and Bohdal, Ondrej and Hospedales, Timothy},
  journal={arXiv preprint arXiv:2403.13164},
  year={2024},
  selected={true},
  code={https://github.com/ys-zong/VL-ICL},
  arxiv={https://ys-zong.github.io/VL-ICL/}
}

@article{zong2023self,
  title={Self-supervised multimodal learning: A survey},
  author={Zong, Yongshuo and Mac Aodha, Oisin and Hospedales, Timothy},
  journal={arXiv preprint arXiv:2304.01008},
  year={2023},
  selected={true},
  code={https://github.com/ys-zong/awesome-self-supervised-multimodal-learning},
  arxiv={https://arxiv.org/abs/2304.01008}
}

@article{zong2022const,
  title={conST: an interpretable multi-modal contrastive learning framework for spatial transcriptomics},
  author={Zong, Yongshuo and Yu, Tingyang and Wang, Xuesong and Wang, Yixuan and Hu, Zhihang and Li, Yu},
  journal={BioRxiv preprint},
  year={2022},
  selected={true},
  code={https://github.com/ys-zong/conST},
  arxiv={https://www.biorxiv.org/content/10.1101/2022.01.14.476408v1}
}

